{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Froze Lake Problem\n",
    "\n",
    "Png Qun Shen\n",
    "\n",
    "A0199519J\n",
    "\n",
    "png.qunshen@u.nus.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Monte_Carlo_without_es import Monte_carlo_without_es\n",
    "from Sarsa import Sarsa\n",
    "from Q_learning import Q_learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: 4 x 4 Grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the default environment for the 4x4 Froze Lake problem for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four by four obstacle\n",
    "four_by_four_obs = [(1,1), (1,3), (2,3), (3,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mont1 = Monte_carlo_without_es(4, 4, obstacle_pos=four_by_four_obs)\n",
    "print(\"Original map:\")\n",
    "print(mont1.get_map())\n",
    "print(\"Number of iteration: {}\".format(mont1.generate_path(10000)))\n",
    "print(\"Solution:\")\n",
    "print(mont1.get_path_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa1 = Sarsa(4, 4, obstacle_pos=four_by_four_obs)\n",
    "print(\"Original map:\")\n",
    "print(sarsa1.get_map())\n",
    "print(\"Number of iteration: {}\".format(sarsa1.generate_path(1000)))\n",
    "print(\"Solution:\")\n",
    "print(sarsa1.get_path_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn1 = Q_learning(4, 4, obstacle_pos=four_by_four_obs)\n",
    "print(\"Original map:\")\n",
    "print(q_learn1.get_map())\n",
    "print(\"Number of iteration: {}\".format(q_learn1.generate_path(1000)))\n",
    "print(\"Solution:\")\n",
    "print(q_learn1.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal parameters (discount rate, epsilon), the parameters are iterated over a few values.\n",
    "\n",
    "For each set of parameters, the problem is solved 30 times, and the number of iterations in each run is recorded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_lst = [i * 0.1 for i in range(1,10)]\n",
    "discount_rate_lst = [i * 0.1 for i in range(1,10)]\n",
    "mont_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        mont_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            mont = Monte_carlo_without_es(4, 4, epsilon=epsilon, \\\n",
    "                                          discount_rate=discount_rate, \\\n",
    "                                            obstacle_pos=four_by_four_obs)\n",
    "            mont_lst = np.append(mont_lst, np.array([mont.generate_path(10000)[0]]), axis = 0)\n",
    "        mean_lst.append(mont_lst.mean())\n",
    "    mont_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Monte Carlo without Exploring Start:\")\n",
    "print(pd.DataFrame(mont_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_lst = [i * 0.1 for i in range(1,10)]\n",
    "discount_rate_lst = [i * 0.1 for i in range(1,10)]\n",
    "sarsa_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        sarsa_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            sarsa = Sarsa(4, 4, epsilon=epsilon, \\\n",
    "                          discount_rate=discount_rate, \\\n",
    "                            obstacle_pos=four_by_four_obs)\n",
    "            sarsa_lst = np.append(sarsa_lst, np.array([sarsa.generate_path(10000)[0]]), axis = 0)\n",
    "        mean_lst.append(sarsa_lst.mean())\n",
    "    sarsa_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Sarsa:\")\n",
    "print(pd.DataFrame(sarsa_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_lst = [i for i in range(1,10)] * 0.1\n",
    "discount_rate_lst = [i for i in range(1,10)] * 0.1\n",
    "q_learn_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        q_learn_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            q_learn = Q_learning(4, 4, epsilon=epsilon, \\\n",
    "                                 discount_rate=discount_rate, \\\n",
    "                                    obstacle_pos=four_by_four_obs)\n",
    "            q_learn_lst = np.append(q_learn_lst, np.array([q_learn.generate_path(1000)[0]]), axis = 0)\n",
    "        mean_lst.append(q_learn_lst.mean())\n",
    "    q_learn_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Q-Learning:\")\n",
    "print(pd.DataFrame(q_learn_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decayed $\\epsilon$-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the optimal discount rate, decayed $\\epsilon$-greedy policy is attempted by setting epsilon value to None (default value).\n",
    "\n",
    "Decayed $\\epsilon$-greedy policy decreases the epsilon value linearly from 1 to 0.1 as the number of iteration increases, encouraging more exploration at the start and more exploitation towards the end.\n",
    "\n",
    "The mean number of iterations over 30 runs are compared between using the optimal epsilon value and using decayed $\\epsilon$-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mont_decayed_lst = np.empty((0), int)\n",
    "mont_epsilon_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    mont_decayed = Monte_carlo_without_es(4, 4, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    mont_epsilon = Monte_carlo_without_es(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "    mont_decayed_lst = np.append(mont_decayed_lst, np.array([mont_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_epsilon_lst = np.append(mont_epsilon_lst, np.array([mont_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Monte Carlo without Exploring Start\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(mont_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(mont_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_decayed_lst = np.empty((0), int)\n",
    "sarsa_epsilon_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    sarsa_decayed = Sarsa(4, 4, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_epsilon = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_decayed_lst = np.append(sarsa_decayed_lst, np.array([sarsa_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_epsilon_lst = np.append(sarsa_epsilon_lst, np.array([sarsa_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Sarsa\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(sarsa_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(sarsa_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn_decayed_lst = np.empty((0), int)\n",
    "q_learn_epsilon_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    q_learn_decayed = Q_learning(4, 4, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_epsilon = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_decayed_lst = np.append(q_learn_decayed_lst, np.array([q_learn_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_epsilon_lst = np.append(q_learn_epsilon_lst, np.array([q_learn_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Q-Learning\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(q_learn_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(q_learn_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Shaping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the optimal discount rate and decayed $\\epsilon$-greedy policy, reward shaping is attempted. There are 2 reward shaping techniques: Manhattan distance and Artificial Potential Field\n",
    "\n",
    "Manhattan distance: the manhattan distance from each point to the goal is calculated, and scaled by dividing by the maximum possible manhattan distance (number of rows + number of columns) such that the value is between 0 and 1. Finally, the value is subtracted from 1 to become the reward for that cell. This generates a higher positive reward the closer the cell is to the goal based on manhattan distance. ($reward = 1 - man\\_dist(cell, goal)/(num\\_row + num\\_col)$)\n",
    "\n",
    "Artificial Potential Field: a potential field is generated such that each hole generate repulsion, and the goal generates attraction. \n",
    "\n",
    "$$att = \\begin{cases}\n",
    "    \\frac{\\alpha}{dist_{man}\\left(cell, goal\\right)}, \n",
    "    & \\text{if}\\ dist_{man}\\left(cell, goal\\right)\\leq max_{cell, goal} \\\\\n",
    "    0, & \\text{if}\\ dist_{man}\\left(cell, goal\\right)> max_{cell, goal}\n",
    "\\end{cases}$$\n",
    "$$rep_{i} = \\begin{cases}\n",
    "    -\\frac{\\beta}{dist_{man}\\left(hole_{i}, cell\\right)^{2}}\\left(\\frac{1}{dist_{man}\\left(hole_{i}, cell\\right)}\n",
    "    -\\frac{1}{dist_{man}\\left(cell, goal\\right)}\\right), & \\text{if}\\ dist_{man}\\left(hole_{i}, cell\\right)\\leq max_{hole_{i}, cell} \\\\\n",
    "    0, & \\text{if}\\ dist_{man}\\left(hole_{i}, cell\\right)> max_{hole_{i}, cell}\n",
    "\\end{cases}$$\n",
    "$$potential = att + \\sum_{i}{rep_{i}}$$\n",
    "\n",
    "The potential at each cell is the reward at the cell. This is done to discourage the robot from going towards the holes, and encourage the robot to go towards the goal (just like using only manhattan distance)\n",
    "\n",
    "The mean number of iterations from 30 runs from using each reward shaping technique is compared to when no reward shaping was used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mont_no_rew_lst = np.empty((0), int)\n",
    "mont_man_lst = np.empty((0), int)\n",
    "mont_apf_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    mont_no_rew = Monte_carlo_without_es(4, 4, discount_rate=0.9, epsilon=None, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    mont_man = Monte_carlo_without_es(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    mont_apf = Monte_carlo_without_es(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    mont_no_rew_lst = np.append(mont_no_rew_lst, np.array([mont_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_man_lst = np.append(mont_man_lst, np.array([mont_man.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_apf_lst = np.append(mont_apf_lst, np.array([mont_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Monte Carlo without Exploring Start\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(mont_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(mont_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(mont_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_no_rew_lst = np.empty((0), int)\n",
    "sarsa_man_lst = np.empty((0), int)\n",
    "sarsa_apf_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    sarsa_no_rew = Sarsa(4, 4, discount_rate=0.9, epsilon=None, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_man = Sarsa(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    sarsa_apf = Sarsa(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    sarsa_no_rew_lst = np.append(sarsa_no_rew_lst, np.array([sarsa_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_man_lst = np.append(sarsa_man_lst, np.array([sarsa_man.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_apf_lst = np.append(sarsa_apf_lst, np.array([sarsa_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Sarsa\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(sarsa_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(sarsa_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(sarsa_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn_no_rew_lst = np.empty((0), int)\n",
    "q_learn_man_lst = np.empty((0), int)\n",
    "q_learn_apf_lst = np.empty((0), int)\n",
    "for i in range(30):\n",
    "    q_learn_no_rew = Q_learning(4, 4, discount_rate=0.9, epsilon=None, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_man = Q_learning(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    q_learn_apf = Q_learning(4, 4, discount_rate=0.9, epsilon=None, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    q_learn_no_rew_lst = np.append(q_learn_no_rew_lst, np.array([q_learn_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_man_lst = np.append(q_learn_man_lst, np.array([q_learn_man.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_apf_lst = np.append(q_learn_apf_lst, np.array([q_learn_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Q Learning\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(q_learn_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(q_learn_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(q_learn_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: 10 x 10 Grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly generate a 10 x 10 enviroment with 25% holes\n",
    "\n",
    "Solve the problem using the parameters found in task 1, printing the original map and the solution\n",
    "\n",
    "X - holes\n",
    "\n",
    "G - goal\n",
    "\n",
    "O - path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mont2 = Monte_carlo_without_es(10, 10, discount_rate=0.9)\n",
    "print(\"Original map:\")\n",
    "print(mont2.get_map())\n",
    "print(\"Number of iteration: {}\".format(mont2.generate_path(100000)))\n",
    "print(\"Solution:\")\n",
    "print(mont2.get_path_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa2 = Sarsa(10, 10, discount_rate=0.8)\n",
    "print(\"Original map:\")\n",
    "print(sarsa2.get_map())\n",
    "print(\"Number of iteration: {}\".format(sarsa2.generate_path(1000000)))\n",
    "print(\"Solution:\")\n",
    "print(sarsa2.get_path_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn2 = Q_learning(10, 10, epsilon=0.5, discount_rate=0.8)\n",
    "print(\"Original map:\")\n",
    "print(q_learn2.get_map())\n",
    "print(\"Number of iteration: {}\".format(q_learn2.generate_path(100000)))\n",
    "print(\"Solution:\")\n",
    "print(q_learn2.get_path_map())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
