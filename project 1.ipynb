{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Froze Lake Problem\n",
    "\n",
    "Png Qun Shen\n",
    "\n",
    "A0199519J\n",
    "\n",
    "png.qunshen@u.nus.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Monte_Carlo_without_es import Monte_carlo_without_es\n",
    "from Sarsa import Sarsa\n",
    "from Q_learning import Q_learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: 4 x 4 Grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the default environment for the 4x4 Froze Lake problem for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four by four obstacle\n",
    "four_by_four_obs = [(1,1), (1,3), (2,3), (3,0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Window Average"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the effects of rolling window average on convergence for Monte Carlo without ES, the experiment is ran 100 times and the average number of iterations required to converge is recorded.\n",
    "\n",
    "A window of 100 is arbitrarily chosen, and it is set in the rolling_window parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo without Exploring Start\n",
      "Mean number of iterations, rolling window: 473.75\n",
      "Mean number of iterations, no rolling window: 1230.33\n"
     ]
    }
   ],
   "source": [
    "mont_rolling_lst = np.empty((0), int)\n",
    "mont_no_rolling_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    mont_rolling = Monte_carlo_without_es(4, 4, rolling_window=100, obstacle_pos=four_by_four_obs)\n",
    "    mont_no_rolling = Monte_carlo_without_es(4, 4, rolling_window=None, obstacle_pos=four_by_four_obs)\n",
    "    mont_rolling_lst = np.append(mont_rolling_lst, np.array([mont_rolling.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_no_rolling_lst = np.append(mont_no_rolling_lst, np.array([mont_no_rolling.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Monte Carlo without Exploring Start\")\n",
    "print(\"Mean number of iterations, rolling window: {}\".format(mont_rolling_lst.mean()))\n",
    "print(\"Mean number of iterations, no rolling window: {}\".format(mont_no_rolling_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal parameters (discount rate, epsilon), the parameters are iterated over a few values.\n",
    "\n",
    "For each set of parameters, the problem is solved 30 times (for a maximum of 1000 iterations each), and the number of iterations in each run is recorded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of iterations for Monte Carlo without Exploring Start:\n",
      "            0.1         0.2         0.3         0.4         0.5         0.6  \\\n",
      "0.1  833.600000  680.333333  593.233333  401.800000  265.900000  320.933333   \n",
      "0.2  835.733333  772.066667  607.766667  408.166667  360.100000  270.500000   \n",
      "0.3  924.666667  860.766667  658.533333  564.966667  294.500000  178.166667   \n",
      "0.4  876.466667  873.200000  768.900000  668.966667  457.233333  393.066667   \n",
      "0.5  999.000000  979.000000  869.633333  688.533333  777.200000  564.233333   \n",
      "0.6  999.000000  999.000000  993.033333  881.966667  750.500000  559.533333   \n",
      "0.7  966.233333  999.000000  981.333333  883.100000  937.733333  679.533333   \n",
      "0.8  999.000000  969.400000  999.000000  946.333333  750.133333  861.400000   \n",
      "0.9  966.866667  874.133333  938.433333  849.800000  811.966667  776.533333   \n",
      "\n",
      "            0.7         0.8         0.9  \n",
      "0.1  375.433333  233.266667  308.166667  \n",
      "0.2  185.333333  121.433333  159.333333  \n",
      "0.3  249.366667  108.800000  146.600000  \n",
      "0.4  212.566667  176.733333   78.400000  \n",
      "0.5  237.433333  167.766667   88.933333  \n",
      "0.6  466.900000  208.100000  105.933333  \n",
      "0.7  522.700000  306.166667  159.366667  \n",
      "0.8  635.666667  430.600000  204.666667  \n",
      "0.9  681.866667  579.366667  282.433333  \n"
     ]
    }
   ],
   "source": [
    "epsilon_lst = [i * 0.1 for i in range(1,10)]\n",
    "discount_rate_lst = [i * 0.1 for i in range(1,10)]\n",
    "mont_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        mont_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            mont = Monte_carlo_without_es(4, 4, rolling_window=100, epsilon=epsilon, \\\n",
    "                                          discount_rate=discount_rate, \\\n",
    "                                            obstacle_pos=four_by_four_obs)\n",
    "            mont_lst = np.append(mont_lst, np.array([mont.generate_path(1000)[0]]), axis = 0)\n",
    "        mean_lst.append(mont_lst.mean())\n",
    "    mont_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Monte Carlo without Exploring Start:\")\n",
    "print(pd.DataFrame(mont_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of iterations for Sarsa:\n",
      "            0.1         0.2         0.3         0.4         0.5         0.6  \\\n",
      "0.1   56.733333   34.000000   45.200000   33.166667   29.133333   30.100000   \n",
      "0.2   53.600000   43.366667   38.766667   36.566667   29.266667   24.900000   \n",
      "0.3   65.466667   54.500000   39.600000   35.966667   26.833333   24.333333   \n",
      "0.4   57.800000   48.866667   43.266667   37.700000   33.633333   30.366667   \n",
      "0.5   90.733333   57.633333   50.766667   38.233333   40.200000   38.033333   \n",
      "0.6   80.966667   71.633333   60.766667   62.433333   36.533333   41.166667   \n",
      "0.7  100.600000   93.433333  100.866667   64.566667   66.066667   65.900000   \n",
      "0.8  163.400000  151.566667  128.100000  108.833333   92.700000   94.600000   \n",
      "0.9  288.633333  224.233333  213.200000  185.900000  156.300000  137.600000   \n",
      "\n",
      "            0.7         0.8         0.9  \n",
      "0.1   22.733333   23.966667   19.633333  \n",
      "0.2   23.366667   18.433333   21.166667  \n",
      "0.3   25.166667   21.933333   23.833333  \n",
      "0.4   31.566667   23.266667   21.600000  \n",
      "0.5   26.666667   32.666667   32.233333  \n",
      "0.6   43.133333   37.100000   40.433333  \n",
      "0.7   56.600000   44.533333   45.766667  \n",
      "0.8   67.333333   69.000000   66.100000  \n",
      "0.9  107.733333  108.266667  108.933333  \n"
     ]
    }
   ],
   "source": [
    "epsilon_lst = [i * 0.1 for i in range(1,10)]\n",
    "discount_rate_lst = [i * 0.1 for i in range(1,10)]\n",
    "sarsa_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        sarsa_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            sarsa = Sarsa(4, 4, epsilon=epsilon, \\\n",
    "                          discount_rate=discount_rate, \\\n",
    "                            obstacle_pos=four_by_four_obs)\n",
    "            sarsa_lst = np.append(sarsa_lst, np.array([sarsa.generate_path(1000)[0]]), axis = 0)\n",
    "        mean_lst.append(sarsa_lst.mean())\n",
    "    sarsa_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Sarsa:\")\n",
    "print(pd.DataFrame(sarsa_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of iterations for Q-Learning:\n",
      "            0.1         0.2         0.3         0.4         0.5         0.6  \\\n",
      "0.1   39.433333   53.800000   42.900000   36.833333   31.200000   30.366667   \n",
      "0.2   60.533333   52.933333   39.766667   34.000000   33.933333   25.433333   \n",
      "0.3   59.733333   52.066667   47.533333   32.600000   32.200000   27.366667   \n",
      "0.4   63.333333   54.566667   45.066667   37.733333   39.500000   35.733333   \n",
      "0.5   82.266667   60.933333   50.233333   49.633333   48.566667   33.366667   \n",
      "0.6   88.966667   69.266667   63.700000   54.033333   51.133333   52.400000   \n",
      "0.7  115.400000   95.366667   75.266667   71.966667   66.500000   61.000000   \n",
      "0.8  163.666667  148.366667  120.866667   94.333333  107.166667  111.366667   \n",
      "0.9  338.566667  262.033333  177.833333  229.400000  221.500000  184.133333   \n",
      "\n",
      "            0.7         0.8         0.9  \n",
      "0.1   22.433333   24.066667   19.266667  \n",
      "0.2   30.133333   22.633333   22.600000  \n",
      "0.3   25.933333   23.000000   21.200000  \n",
      "0.4   31.800000   27.633333   24.000000  \n",
      "0.5   31.533333   33.566667   37.400000  \n",
      "0.6   48.200000   37.900000   38.600000  \n",
      "0.7   56.233333   50.566667   51.566667  \n",
      "0.8   99.966667   77.733333   75.266667  \n",
      "0.9  180.866667  162.800000  148.900000  \n"
     ]
    }
   ],
   "source": [
    "epsilon_lst = [i * 0.1 for i in range(1,10)]\n",
    "discount_rate_lst = [i * 0.1 for i in range(1,10)]\n",
    "q_learn_mean = []\n",
    "for epsilon in epsilon_lst: # loop through epsilon\n",
    "    mean_lst = []\n",
    "    for discount_rate in discount_rate_lst: # loop through discount_rate\n",
    "        q_learn_lst = np.empty((0), int)\n",
    "\n",
    "        # loop 30 times\n",
    "        for i in range(30):\n",
    "            q_learn = Q_learning(4, 4, epsilon=epsilon, \\\n",
    "                                 discount_rate=discount_rate, \\\n",
    "                                    obstacle_pos=four_by_four_obs)\n",
    "            q_learn_lst = np.append(q_learn_lst, np.array([q_learn.generate_path(1000)[0]]), axis = 0)\n",
    "        mean_lst.append(q_learn_lst.mean())\n",
    "    q_learn_mean.append(mean_lst)\n",
    "print(\"Mean number of iterations for Q-Learning:\")\n",
    "print(pd.DataFrame(q_learn_mean, index=epsilon_lst, columns=discount_rate_lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decayed $\\epsilon$-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the optimal discount rate, decayed $\\epsilon$-greedy policy is attempted by setting epsilon value to None (default value).\n",
    "\n",
    "Decayed $\\epsilon$-greedy policy decreases the epsilon value linearly from 1 to 0.1 as the number of iteration increases, encouraging more exploration at the start and more exploitation towards the end.\n",
    "\n",
    "The mean number of iterations over 100 runs are compared between using the optimal epsilon value and using decayed $\\epsilon$-greedy policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo without Exploring Start\n",
      "Mean number of iterations, decayed epsilon-greedy: 507.97\n",
      "Mean number of iterations, optimal epsilon: 86.85\n"
     ]
    }
   ],
   "source": [
    "mont_decayed_lst = np.empty((0), int)\n",
    "mont_epsilon_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    mont_decayed = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    mont_epsilon = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=0.4, obstacle_pos=four_by_four_obs)\n",
    "    mont_decayed_lst = np.append(mont_decayed_lst, np.array([mont_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_epsilon_lst = np.append(mont_epsilon_lst, np.array([mont_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Monte Carlo without Exploring Start\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(mont_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(mont_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarsa\n",
      "Mean number of iterations, decayed epsilon-greedy: 167.23\n",
      "Mean number of iterations, optimal epsilon: 17.74\n"
     ]
    }
   ],
   "source": [
    "sarsa_decayed_lst = np.empty((0), int)\n",
    "sarsa_epsilon_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    sarsa_decayed = Sarsa(4, 4, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_epsilon = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_decayed_lst = np.append(sarsa_decayed_lst, np.array([sarsa_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_epsilon_lst = np.append(sarsa_epsilon_lst, np.array([sarsa_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Sarsa\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(sarsa_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(sarsa_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning\n",
      "Mean number of iterations, decayed epsilon-greedy: 238.72\n",
      "Mean number of iterations, optimal epsilon: 19.34\n"
     ]
    }
   ],
   "source": [
    "q_learn_decayed_lst = np.empty((0), int)\n",
    "q_learn_epsilon_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    q_learn_decayed = Q_learning(4, 4, discount_rate=0.9, epsilon=None, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_epsilon = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_decayed_lst = np.append(q_learn_decayed_lst, np.array([q_learn_decayed.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_epsilon_lst = np.append(q_learn_epsilon_lst, np.array([q_learn_epsilon.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Q-Learning\")\n",
    "print(\"Mean number of iterations, decayed epsilon-greedy: {}\".format(q_learn_decayed_lst.mean()))\n",
    "print(\"Mean number of iterations, optimal epsilon: {}\".format(q_learn_epsilon_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Shaping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the optimal discount rate and decayed $\\epsilon$-greedy policy, reward shaping is attempted. There are 2 reward shaping techniques: Manhattan distance and Artificial Potential Field\n",
    "\n",
    "Manhattan distance: the manhattan distance from each point to the goal is calculated, and scaled by dividing by the maximum possible manhattan distance (number of rows + number of columns) such that the value is between 0 and 1. Finally, the value is subtracted from 1 to become the reward for that cell. This generates a higher positive reward the closer the cell is to the goal based on manhattan distance. \n",
    "\n",
    "$$reward = 1 - dist_{man}\\left(cell, goal\\right)/\\left(num\\_row + num\\_col\\right)$$\n",
    "\n",
    "Artificial Potential Field: a potential field is generated such that each hole generate repulsion, and the goal generates attraction. \n",
    "\n",
    "$$att = \\begin{cases}\n",
    "    \\frac{\\alpha}{dist_{man}\\left(cell, goal\\right)}, \n",
    "    & \\text{if}\\ dist_{man}\\left(cell, goal\\right)\\leq max_{cell, goal} \\\\\n",
    "    0, & \\text{if}\\ dist_{man}\\left(cell, goal\\right)> max_{cell, goal}\n",
    "\\end{cases}$$\n",
    "$$rep_{i} = \\begin{cases}\n",
    "    -\\frac{\\beta}{dist_{man}\\left(hole_{i}, cell\\right)^{2}}\\left(\\frac{1}{dist_{man}\\left(hole_{i}, cell\\right)}\n",
    "    -\\frac{1}{max_{hole, cell}}\\right), & \\text{if}\\ dist_{man}\\left(hole_{i}, cell\\right)\\leq max_{hole_{i}, cell} \\\\\n",
    "    0, & \\text{if}\\ dist_{man}\\left(hole_{i}, cell\\right)> max_{hole_{i}, cell}\n",
    "\\end{cases}$$\n",
    "$$potential = att + \\sum_{i}{rep_{i}}$$\n",
    "\n",
    "The potential at each cell is the reward at the cell. This is done to discourage the robot from going towards the holes, and encourage the robot to go towards the goal (just like using only manhattan distance)\n",
    "\n",
    "The mean number of iterations from 100 runs from using each reward shaping technique is compared to when no reward shaping was used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo without Exploring Start\n",
      "Mean number of iterations, No reward shaping: 117.77\n",
      "Mean number of iterations, Manhattan distance: 86.99\n",
      "Mean number of iterations, Artificial Potential Field: 111.79\n"
     ]
    }
   ],
   "source": [
    "mont_no_rew_lst = np.empty((0), int)\n",
    "mont_man_lst = np.empty((0), int)\n",
    "mont_apf_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    mont_no_rew = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=0.4, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    mont_man = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=0.4, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    mont_apf = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=0.4, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    mont_no_rew_lst = np.append(mont_no_rew_lst, np.array([mont_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_man_lst = np.append(mont_man_lst, np.array([mont_man.generate_path(10000)[0]]), axis = 0)\n",
    "    mont_apf_lst = np.append(mont_apf_lst, np.array([mont_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Monte Carlo without Exploring Start\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(mont_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(mont_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(mont_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarsa\n",
      "Mean number of iterations, No reward shaping: 18.01\n",
      "Mean number of iterations, Manhattan distance: 9999.0\n",
      "Mean number of iterations, Artificial Potential Field: 9540.49\n"
     ]
    }
   ],
   "source": [
    "sarsa_no_rew_lst = np.empty((0), int)\n",
    "sarsa_man_lst = np.empty((0), int)\n",
    "sarsa_apf_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    sarsa_no_rew = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    sarsa_man = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    sarsa_apf = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    sarsa_no_rew_lst = np.append(sarsa_no_rew_lst, np.array([sarsa_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_man_lst = np.append(sarsa_man_lst, np.array([sarsa_man.generate_path(10000)[0]]), axis = 0)\n",
    "    sarsa_apf_lst = np.append(sarsa_apf_lst, np.array([sarsa_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Sarsa\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(sarsa_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(sarsa_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(sarsa_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning\n",
      "Mean number of iterations, No reward shaping: 19.84\n",
      "Mean number of iterations, Manhattan distance: 9999.0\n",
      "Mean number of iterations, Artificial Potential Field: 9999.0\n"
     ]
    }
   ],
   "source": [
    "q_learn_no_rew_lst = np.empty((0), int)\n",
    "q_learn_man_lst = np.empty((0), int)\n",
    "q_learn_apf_lst = np.empty((0), int)\n",
    "for i in range(100):\n",
    "    q_learn_no_rew = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=None, obstacle_pos=four_by_four_obs)\n",
    "    q_learn_man = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "    q_learn_apf = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, reward_shape=\"apf\", obstacle_pos=four_by_four_obs)\n",
    "    q_learn_no_rew_lst = np.append(q_learn_no_rew_lst, np.array([q_learn_no_rew.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_man_lst = np.append(q_learn_man_lst, np.array([q_learn_man.generate_path(10000)[0]]), axis = 0)\n",
    "    q_learn_apf_lst = np.append(q_learn_apf_lst, np.array([q_learn_apf.generate_path(10000)[0]]), axis = 0)\n",
    "print(\"Q Learning\")\n",
    "print(\"Mean number of iterations, No reward shaping: {}\".format(q_learn_no_rew_lst.mean()))\n",
    "print(\"Mean number of iterations, Manhattan distance: {}\".format(q_learn_man_lst.mean()))\n",
    "print(\"Mean number of iterations, Artificial Potential Field: {}\".format(q_learn_apf_lst.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 61\n",
      "Solution:\n",
      "==============\n",
      "| O  O  O    |\n",
      "|    X  O  X |\n",
      "|       O  X |\n",
      "| X     O  G |\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "mont1 = Monte_carlo_without_es(4, 4, rolling_window=100, discount_rate=0.9, epsilon=0.4, reward_shape=\"manhattan\", obstacle_pos=four_by_four_obs)\n",
    "print(\"Number of iteration: {}\".format(mont1.generate_path(10000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(mont1.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 16\n",
      "Solution:\n",
      "==============\n",
      "| O          |\n",
      "| O  X     X |\n",
      "| O  O     X |\n",
      "| X  O  O  G |\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "sarsa1 = Sarsa(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "print(\"Number of iteration: {}\".format(sarsa1.generate_path(1000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(sarsa1.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iteration: 9\n",
      "Solution:\n",
      "==============\n",
      "| O          |\n",
      "| O  X     X |\n",
      "| O  O     X |\n",
      "| X  O  O  G |\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "q_learn1 = Q_learning(4, 4, discount_rate=0.9, epsilon=0.1, obstacle_pos=four_by_four_obs)\n",
    "print(\"Number of iteration: {}\".format(q_learn1.generate_path(1000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(q_learn1.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: 10 x 10 Grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly generate a 10 x 10 enviroment with 25% holes\n",
    "\n",
    "Solve the problem using the parameters found in task 1, printing the original map and the solution\n",
    "\n",
    "X - holes\n",
    "\n",
    "G - goal\n",
    "\n",
    "O - path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo without Exploring Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original map:\n",
      "================================\n",
      "|          X     X        X  X |\n",
      "|    X        X        X       |\n",
      "|       X        X     X  X  X |\n",
      "| X                          X |\n",
      "|    X        X     X          |\n",
      "|                              |\n",
      "|                              |\n",
      "| X     X        X  X          |\n",
      "|             X           X    |\n",
      "| X                    X     G |\n",
      "================================\n",
      "Number of iteration: 44321\n",
      "Solution:\n",
      "================================\n",
      "| O  O  O  X     X        X  X |\n",
      "|    X  O  O  X        X       |\n",
      "|       X  O     X     X  X  X |\n",
      "| X        O                 X |\n",
      "|    X     O  X     X          |\n",
      "|          O     O  O  O  O  O |\n",
      "|          O  O  O           O |\n",
      "| X     X        X  X        O |\n",
      "|             X           X  O |\n",
      "| X                    X     G |\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "mont2 = Monte_carlo_without_es(10, 10, rolling_window=100, discount_rate=0.9, epsilon=0.4, reward_shape=\"manhattan\")\n",
    "print(\"Original map:\")\n",
    "print(mont2.get_map())\n",
    "print(\"Number of iteration: {}\".format(mont2.generate_path(1000000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(mont2.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original map:\n",
      "================================\n",
      "|                              |\n",
      "|                              |\n",
      "| X           X     X          |\n",
      "| X  X  X     X           X    |\n",
      "| X        X           X     X |\n",
      "|             X        X  X    |\n",
      "|          X              X  X |\n",
      "|          X                   |\n",
      "|       X           X  X     X |\n",
      "|                X     X     G |\n",
      "================================\n",
      "Number of iteration: 335\n",
      "Solution:\n",
      "================================\n",
      "| O  O  O  O  O                |\n",
      "|             O  O             |\n",
      "| X           X  O  X          |\n",
      "| X  X  X     X  O        X    |\n",
      "| X        X     O     X     X |\n",
      "|             X  O     X  X    |\n",
      "|          X     O  O  O  X  X |\n",
      "|          X           O  O    |\n",
      "|       X           X  X  O  X |\n",
      "|                X     X  O  G |\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "sarsa2 = Sarsa(10, 10, discount_rate=0.9, epsilon=0.1, reward_shape=None)\n",
    "print(\"Original map:\")\n",
    "print(sarsa2.get_map())\n",
    "print(\"Number of iteration: {}\".format(sarsa2.generate_path(1000000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(sarsa2.get_path_map())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original map:\n",
      "================================\n",
      "|                X     X     X |\n",
      "| X                          X |\n",
      "| X           X     X     X    |\n",
      "|    X  X                      |\n",
      "|       X                 X    |\n",
      "|    X                 X       |\n",
      "|    X  X        X             |\n",
      "| X        X                 X |\n",
      "|             X        X       |\n",
      "|    X           X           G |\n",
      "================================\n",
      "Number of iteration: 322\n",
      "Solution:\n",
      "================================\n",
      "| O  O           X     X     X |\n",
      "| X  O                       X |\n",
      "| X  O  O  O  X     X     X    |\n",
      "|    X  X  O  O                |\n",
      "|       X     O           X    |\n",
      "|    X        O        X       |\n",
      "|    X  X     O  X             |\n",
      "| X        X  O  O           X |\n",
      "|             X  O  O  X       |\n",
      "|    X           X  O  O  O  G |\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "q_learn2 = Q_learning(10, 10, discount_rate=0.9, epsilon=0.1, reward_shape=None)\n",
    "print(\"Original map:\")\n",
    "print(q_learn2.get_map())\n",
    "print(\"Number of iteration: {}\".format(q_learn2.generate_path(1000000)[0]))\n",
    "print(\"Solution:\")\n",
    "print(q_learn2.get_path_map())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
