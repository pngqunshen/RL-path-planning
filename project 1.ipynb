{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo without Exploring Start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Usage: Create a Monte_carlo_without_es object, and call the generate_path() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_carlo_without_es():\n",
    "    def __init__(self, row, col, obstacle_pos = None, goal_pos = None, discount_rate = 0.9, epsilon = 0.2, seed = 1):\n",
    "        # model params\n",
    "        self.row = row # number of rows of grid\n",
    "        self.col = col # number of columns of grid\n",
    "        self.num_actions = 4\n",
    "\n",
    "        # initialise model\n",
    "        self.reward = self.initialise_reward(obstacle_pos, goal_pos, seed)\n",
    "        self.policy = self.initialise_policy()\n",
    "        self.q_value = self.initialise_q_value()\n",
    "        self.g_list = {}\n",
    "\n",
    "        # other params\n",
    "        self.discount_rate = discount_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def initialise_reward(self, obstacle_pos, goal_pos, seed):\n",
    "        reward = np.ones((self.row,self.col))*-0.04\n",
    "\n",
    "        # if obstacle not given, randomise 25% of tiles to be obstacles\n",
    "        random.seed(seed)\n",
    "        if obstacle_pos == None:\n",
    "            for i in range(int(0.25 * self.row * self.col)):\n",
    "                while True:\n",
    "                    rand_i = random.randint(0, self.row-1)\n",
    "                    rand_j = random.randint(0, self.col-1)\n",
    "                    if reward[rand_i, rand_j] != -1 and reward[rand_i, rand_j] != 1: \n",
    "                        reward[rand_i, rand_j] = -1\n",
    "                        break\n",
    "        else: # use given obstacles\n",
    "            for (i, j) in obstacle_pos:\n",
    "                reward[i,j] = -1\n",
    "        # if goal not given, default to lower right\n",
    "        if goal_pos == None:\n",
    "            reward[self.row-1, self.col-1] = 1\n",
    "        else:\n",
    "            reward[goal_pos[0], goal_pos[1]] = 1\n",
    "        return reward\n",
    "\n",
    "    def initialise_policy(self):\n",
    "        pol = np.ones((self.row, self.col, self.num_actions))*(1.0/self.num_actions)\n",
    "        # remove probabilty of going into the wall\n",
    "        pol[0,:] = np.repeat([[0, 1.0/3, 1.0/3, 1.0/3]], self.col, axis=0)\n",
    "        pol[self.row-1,:] = np.repeat([[1.0/3, 1.0/3, 0, 1.0/3]], self.col, axis=0)\n",
    "        pol[:,0] = np.repeat([[1.0/3, 1.0/3, 1.0/3, 0]], self.col, axis=0)\n",
    "        pol[:,self.col-1] = np.repeat([[1.0/3, 0, 1.0/3, 1.0/3]], self.col, axis=0)\n",
    "        # update probability of corner\n",
    "        pol[0,0] = [0, 1.0/2, 1.0/2, 0]\n",
    "        pol[0,self.col-1] = [0, 0, 1.0/2, 1.0/2]\n",
    "        pol[self.row-1,0] = [1.0/2, 1.0/2, 0, 0]\n",
    "        pol[self.row-1,self.col-1] = [1.0/2, 0, 0, 1.0/2]\n",
    "        return pol\n",
    "    \n",
    "    def initialise_q_value(self):\n",
    "        q_value = np.zeros((self.row, self.col, self.num_actions))\n",
    "        # set q_value of going into the wall to low number\n",
    "        q_value[0,:,0] = -2147483648\n",
    "        q_value[:,self.col-1,1] = -2147483648\n",
    "        q_value[self.row-1,:2] = -2147483648\n",
    "        q_value[:,0,3] = -2147483648\n",
    "        return q_value\n",
    "\n",
    "    def generate_episode(self):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        ep = np.empty((0,4), int)\n",
    "        while not (self.reward[i,j] == -1 or self.reward[i,j] == 1):\n",
    "            pol = self.find_policy_to_use(i, j)\n",
    "            next_i = i\n",
    "            next_j = j\n",
    "            if pol == 0:\n",
    "                next_i = max(i-1, 0)\n",
    "            elif pol == 1:\n",
    "                next_j = min(j+1, self.col-1)\n",
    "            elif pol == 2:\n",
    "                next_i = min(i+1, self.row-1)\n",
    "            else:\n",
    "                next_j = max(j-1, 0)\n",
    "            ep = np.append(ep, np.array([[i, j, pol, self.reward[next_i, next_j]]], int), axis=0)\n",
    "            i = next_i\n",
    "            j = next_j\n",
    "        return ep\n",
    "            \n",
    "    def find_policy_to_use(self, i, j):\n",
    "        # random number to generate policy\n",
    "        pol = self.policy[i, j]\n",
    "        rand = random.random()\n",
    "        for k in range(len(pol)):\n",
    "            if rand <= pol[k]:\n",
    "                return k\n",
    "            rand -= pol[k]\n",
    "        return None # should not reach here\n",
    "\n",
    "    def update_path(self):\n",
    "        ep = self.generate_episode()\n",
    "        g = 0\n",
    "        first_visit = np.zeros((self.row, self.col, self.num_actions))\n",
    "        for i in range(len(ep)):\n",
    "            S = ep[i, 0:2]\n",
    "            A = ep[i, 2]\n",
    "            if first_visit[S[0], S[1], A] == 0:\n",
    "                first_visit[S[0], S[1], A] = i\n",
    "        for i in range(len(ep) - 1, -1, -1):\n",
    "            S = ep[i, 0:2]\n",
    "            A = ep[i, 2]\n",
    "            R = ep[i, 3]\n",
    "            g = g * self.discount_rate + R\n",
    "            if first_visit[S[0], S[1], A] == i:\n",
    "                if ((S[0], S[1]), A) not in self.g_list:\n",
    "                    self.g_list[((S[0], S[1]), A)] = []\n",
    "                self.g_list[((S[0], S[1]), A)].append(g)\n",
    "                self.q_value[S[0], S[1], A] = \\\n",
    "                    sum(self.g_list[((S[0], S[1]), A)])/len(self.g_list[((S[0], S[1]), A)])\n",
    "                self.epsilon_greedy(S[0], S[1])\n",
    "    \n",
    "    def epsilon_greedy(self, i, j):\n",
    "        # find total actions\n",
    "        permitted_actions = []\n",
    "        if i > 0: permitted_actions.append(0)\n",
    "        if j < self.col - 1: permitted_actions.append(1)\n",
    "        if i < self.row - 1: permitted_actions.append(2)\n",
    "        if j > 0: permitted_actions.append(3)\n",
    "        # set policy\n",
    "        for act in permitted_actions:\n",
    "            self.policy[i, j, act] = self.epsilon/len(permitted_actions)\n",
    "        a_star = self.q_value[i, j].argmax()\n",
    "        self.policy[i, j, a_star] = \\\n",
    "            1 - self.epsilon + self.epsilon/len(permitted_actions)\n",
    "                        \n",
    "    def generate_path(self):\n",
    "        ep = self.generate_episode()\n",
    "        while True:\n",
    "            self.update_path()\n",
    "            new_ep = self.generate_episode()\n",
    "            print(ep)\n",
    "            if np.array_equal(ep, new_ep) and ep[-1][3] == 1:\n",
    "                break\n",
    "            ep = new_ep\n",
    "        return ep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 x 4 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04 -0.04]\n",
      " [-0.04 -1.   -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-1.   -0.04 -0.04  1.  ]]\n",
      "[[0 0 1 0]\n",
      " [0 1 1 0]\n",
      " [0 2 2 0]\n",
      " [1 2 2 0]\n",
      " [2 2 2 0]\n",
      " [3 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "mont = Monte_carlo_without_es(4, 4, obstacle_pos=[(1,1), (1,3), (2,3), (3,0)])\n",
    "print(mont.reward)\n",
    "# mont.update_path()\n",
    "# print(mont.policy[1])\n",
    "# print(mont.generate_episode())\n",
    "# print(mont.q_value[1][0])\n",
    "print(mont.generate_path())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 x 10 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.   -0.04 -0.04 -0.04 -0.04 -0.04 -1.   -1.   -0.04 -0.04]\n",
      " [-0.04 -0.04 -1.   -0.04 -1.   -0.04 -0.04 -1.   -0.04 -0.04]\n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04 -1.   -0.04 -0.04 -1.   -0.04 -0.04]\n",
      " [-1.   -1.   -0.04 -1.   -0.04 -0.04 -0.04 -0.04 -0.04 -1.  ]\n",
      " [-1.   -0.04 -0.04 -1.   -0.04 -0.04 -0.04 -0.04 -1.   -0.04]\n",
      " [-1.   -0.04 -0.04 -1.   -0.04 -0.04 -0.04 -0.04 -1.   -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -1.   -1.   -0.04]\n",
      " [-1.   -0.04 -0.04 -1.   -0.04 -0.04 -0.04 -0.04 -0.04 -0.04]\n",
      " [-0.04 -1.   -0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  1.  ]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mont\u001b[39m=\u001b[39m Monte_carlo_without_es(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(mont\u001b[39m.\u001b[39mreward)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(mont\u001b[39m.\u001b[39;49mgenerate_path())\n",
      "Cell \u001b[0;32mIn[19], line 136\u001b[0m, in \u001b[0;36mMonte_carlo_without_es.generate_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_path()\n\u001b[1;32m    135\u001b[0m new_ep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_episode()\n\u001b[0;32m--> 136\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(ep, new_ep) \u001b[39mand\u001b[39;00m ep[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    138\u001b[0m ep \u001b[39m=\u001b[39m new_ep\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "mont= Monte_carlo_without_es(10, 10)\n",
    "print(mont.reward)\n",
    "print(mont.generate_path())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Usage: Create a Monte_carlo_without_es object, and call the generate_path() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa():\n",
    "    def __init__(self, row, col, obstacle_pos = None, goal_pos = None, \\\n",
    "                 discount_rate = 0.9, epsilon = 0.1, alpha = 0.1, seed = 1):\n",
    "        # model params\n",
    "        self.row = row # number of rows of grid\n",
    "        self.col = col # number of columns of grid\n",
    "        self.num_actions = 4\n",
    "\n",
    "        # set reward for model\n",
    "        self.reward = np.ones((self.row,self.col))*-0.04\n",
    "\n",
    "        # if obstacle not given, randomise 25% of tiles to be obstacles\n",
    "        random.seed(seed)\n",
    "        if obstacle_pos == None:\n",
    "            for i in range(int(0.25 * self.row * self.col)):\n",
    "                while True:\n",
    "                    rand_i = random.randint(0, row-1)\n",
    "                    rand_j = random.randint(0, col-1)\n",
    "                    if self.reward[rand_i, rand_j] != -1 and self.reward[rand_i, rand_j] != 1: \n",
    "                        self.reward[rand_i, rand_j] = -1\n",
    "                        break\n",
    "        else: # use given obstacles\n",
    "            for (i, j) in obstacle_pos:\n",
    "                self.reward[i,j] = -1\n",
    "        # if goal not given, default to lower right\n",
    "        if goal_pos == None:\n",
    "            self.reward[row-1, col-1] = 1\n",
    "        else:\n",
    "            self.reward[goal_pos[0], goal_pos[1]] = 1\n",
    "\n",
    "        self.policy = self.initialise_policy()\n",
    "        # initialise value to 0\n",
    "        self.q_value = np.zeros((self.row, self.col, self.num_actions))\n",
    "\n",
    "        # other params\n",
    "        self.discount_rate = discount_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def initialise_policy(self):\n",
    "        pol = np.ones((self.row, self.col, self.num_actions))*(1.0/self.num_actions)\n",
    "        # remove probabilty of going into the wall\n",
    "        pol[0,:] = np.repeat([[0, 1.0/3, 1.0/3, 1.0/3]], self.col, axis=0)\n",
    "        pol[self.row-1,:] = np.repeat([[1.0/3, 1.0/3, 0, 1.0/3]], self.col, axis=0)\n",
    "        pol[:,0] = np.repeat([[1.0/3, 1.0/3, 1.0/3, 0]], self.col, axis=0)\n",
    "        pol[:,self.col-1] = np.repeat([[1.0/3, 0, 1.0/3, 1.0/3]], self.col, axis=0)\n",
    "        # update probability of corner\n",
    "        pol[0,0] = [0, 1.0/2, 1.0/2, 0]\n",
    "        pol[0,self.col-1] = [0, 0, 1.0/2, 1.0/2]\n",
    "        pol[self.row-1,0] = [1.0/2, 1.0/2, 0, 0]\n",
    "        pol[self.row-1,self.col-1] = [1.0/2, 0, 0, 1.0/2]\n",
    "        return pol\n",
    "\n",
    "    def generate_episode(self):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        ep = np.empty((0,7), int)\n",
    "        while not (self.reward[i,j] == -1 or self.reward[i,j] == 1):\n",
    "            pol = self.find_policy_to_use(i, j)\n",
    "            next_i = i\n",
    "            next_j = j\n",
    "            if pol == 0:\n",
    "                next_i = max(i-1, 0)\n",
    "            elif pol == 1:\n",
    "                next_j = min(j+1, self.col-1)\n",
    "            elif pol == 2:\n",
    "                next_i = min(i+1, self.row-1)\n",
    "            else:\n",
    "                next_j = max(j-1, 0)\n",
    "            next_pol = self.find_policy_to_use(next_i, next_j)\n",
    "            ep = np.append(ep, np.array([[i, j, pol, self.reward[next_i, next_j], \\\n",
    "                                          next_i, next_j, next_pol]], int), axis=0)\n",
    "            i = next_i\n",
    "            j = next_j\n",
    "        return ep\n",
    "            \n",
    "    def find_policy_to_use(self, i, j):\n",
    "        # random number to generate policy\n",
    "        pol = self.policy[i, j]\n",
    "        rand = random.random()\n",
    "        for k in range(len(pol)):\n",
    "            if rand <= pol[k]:\n",
    "                return k\n",
    "            rand -= pol[k]\n",
    "        return None # should not reach here\n",
    "    \n",
    "    def update_path(self):\n",
    "        ep = self.generate_episode()\n",
    "        for i in range(len(ep)):\n",
    "            S = ep[i, 0:2]\n",
    "            A = ep[i, 2]\n",
    "            R = ep[i, 3]\n",
    "            SN = ep[i, 4:6]\n",
    "            AN = ep[i, 6]\n",
    "            self.q_value[S[0], S[1], A] += \\\n",
    "                (self.alpha*(R+self.discount_rate*self.q_value[SN[0], SN[1], AN] \\\n",
    "                             -self.q_value[S[0], S[1], A]))\n",
    "            self.epsilon_greedy(S[0], S[1])\n",
    "            \n",
    "    def epsilon_greedy(self, i, j):\n",
    "        a_star = self.q_value[i, j].argmax()\n",
    "        # find total actions\n",
    "        a_total = 0\n",
    "        if i > 0: a_total+=1\n",
    "        if i < self.row - 1: a_total+=1\n",
    "        if j > 0: a_total+=1\n",
    "        if j < self.col - 1: a_total+=1\n",
    "        # set policy\n",
    "        if i > 0:\n",
    "            self.policy[i, j, 0] = self.epsilon/a_total\n",
    "        if j < self.col - 1:\n",
    "            self.policy[i, j, 1] = self.epsilon/a_total\n",
    "        if i < self.row - 1:\n",
    "            self.policy[i, j, 2] = self.epsilon/a_total\n",
    "        if j > 0:\n",
    "            self.policy[i, j, 3] = self.epsilon/a_total\n",
    "        self.policy[i, j, a_star] = \\\n",
    "            1 - self.epsilon + self.epsilon/a_total\n",
    "    \n",
    "    def generate_path(self):\n",
    "        ep = self.generate_episode()\n",
    "        while True:\n",
    "            self.update_path()\n",
    "            new_ep = self.generate_episode()\n",
    "            if np.array_equal(ep, new_ep) and ep[-1][3] == 1:\n",
    "                break\n",
    "            ep = new_ep\n",
    "        return ep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 x 4 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04 -0.04]\n",
      " [-0.04 -1.   -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-1.   -0.04 -0.04  1.  ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sarsa \u001b[39m=\u001b[39m Sarsa(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, obstacle_pos\u001b[39m=\u001b[39m[(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), (\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m), (\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m), (\u001b[39m3\u001b[39m,\u001b[39m0\u001b[39m)])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sarsa\u001b[39m.\u001b[39mreward)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(sarsa\u001b[39m.\u001b[39;49mgenerate_path()) \n",
      "Cell \u001b[0;32mIn[17], line 124\u001b[0m, in \u001b[0;36mSarsa.generate_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_path()\n\u001b[0;32m--> 124\u001b[0m     new_ep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_episode()\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(ep, new_ep) \u001b[39mand\u001b[39;00m ep[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    126\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 71\u001b[0m, in \u001b[0;36mSarsa.generate_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     next_j \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(j\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     70\u001b[0m next_pol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_policy_to_use(next_i, next_j)\n\u001b[0;32m---> 71\u001b[0m ep \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(ep, np\u001b[39m.\u001b[39;49marray([[i, j, pol, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward[next_i, next_j], \\\n\u001b[1;32m     72\u001b[0m                               next_i, next_j, next_pol]], \u001b[39mint\u001b[39;49m), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     73\u001b[0m i \u001b[39m=\u001b[39m next_i\n\u001b[1;32m     74\u001b[0m j \u001b[39m=\u001b[39m next_j\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/function_base.py:4700\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4698\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[1;32m   4699\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 4700\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sarsa = Sarsa(4, 4, obstacle_pos=[(1,1), (1,3), (2,3), (3,0)])\n",
    "print(sarsa.reward)\n",
    "print(sarsa.generate_path()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 x 10 Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. -1.  0. -1.  0.  0.  0.  0.]\n",
      " [-1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0. -1.  0. -1.  0.  0.  0.]\n",
      " [-1.  0.  0. -1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [-1.  0.  0.  0.  0. -1.  0. -1. -1. -1.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.  0. -1.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0. -1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.  0.  0.  1.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sarsa \u001b[39m=\u001b[39m Sarsa(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sarsa\u001b[39m.\u001b[39mreward)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(sarsa\u001b[39m.\u001b[39;49mgenerate_path())\n",
      "Cell \u001b[0;32mIn[25], line 90\u001b[0m, in \u001b[0;36mSarsa.generate_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_path()\n\u001b[0;32m---> 90\u001b[0m     new_ep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_episode()\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(ep, new_ep) \u001b[39mand\u001b[39;00m ep[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     92\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 51\u001b[0m, in \u001b[0;36mSarsa.generate_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     next_j \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(j\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     50\u001b[0m next_pol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_policy_to_use(next_i, next_j)\n\u001b[0;32m---> 51\u001b[0m ep \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(ep, np\u001b[39m.\u001b[39;49marray([[i, j, pol, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward[next_i, next_j], \\\n\u001b[1;32m     52\u001b[0m                               next_i, next_j, next_pol]], \u001b[39mint\u001b[39;49m), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m i \u001b[39m=\u001b[39m next_i\n\u001b[1;32m     54\u001b[0m j \u001b[39m=\u001b[39m next_j\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/function_base.py:4700\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4698\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[1;32m   4699\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 4700\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sarsa = Sarsa(10, 10)\n",
    "print(sarsa.reward)\n",
    "print(sarsa.generate_path())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
